# -*- coding: utf-8 -*-
"""Python Web Scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d8J2wpTy650TPfSFFmP6dtS1IlY0t7rr

# **Python Web Scraping (Introduction)**

Web scraping is the process of extracting data from websites. It involves using a tool or script to send HTTP requests to a website, parse the HTML, and extract the desired information

libraries: Install the necessary Python libraries:

* **requests:** To send HTTP requests.
* **BeautifulSoup:** To parse HTML and extract data.
* **pandas:** To store and manipulate the data (optional, for structured data)
"""

pip install requests beautifulsoup4 pandas

"""# **Step 1: Import Necessary Libraries**"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

"""# **Step 2: Send a Request to the Web Page**"""

# Use the requests library to fetch the content of the webpage.

# We'll send a request to fetch the content from http://quotes.toscrape.com.

url = 'http://quotes.toscrape.com'
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    print("Successfully fetched the web page!")
else:
    print(f"Failed to fetch the web page. Status Code: {response.status_code}")

"""Explanation:
* requests.get(url) sends an HTTP GET request to the URL.
* response.status_code checks if the request was successful (status code 200 indicates success).

# **Step 3: Parse the HTML Content**

We'll use BeautifulSoup to parse the HTML content of the webpage.
"""

soup = BeautifulSoup(response.text, 'html.parser')

# Print the title of the page
print(soup.title.string)

"""Explanation:
* BeautifulSoup(response.text, 'html.parser') parses the HTML content.
* soup.title.string extracts and prints the title of the page.

# **Step 4: Find and Extract Data**

We'll extract quotes, authors, and tags from the webpage.
"""

quotes_data = []

# Find all quote blocks
quotes = soup.find_all('div', class_='quote')

for quote in quotes:
    text = quote.find('span', class_='text').text
    author = quote.find('small', class_='author').text
    tags = [tag.text for tag in quote.find_all('a', class_='tag')]

    quotes_data.append({
        'Quote': text,
        'Author': author,
        'Tags': ', '.join(tags)
    })

# Display the extracted data
for data in quotes_data:
    print(data)

"""Explanation:
* soup.find_all('div', class_='quote') finds all div elements with the class quote.
* quote.find('span', class_='text').text extracts the quote text.
* quote.find('small', class_='author').text extracts the author's name.
* quote.find_all('a', class_='tag') extracts all tags associated with the quote.

# **Step 5: Storing Data in a DataFrame**

We'll store the extracted data in a pandas DataFrame and save it to a CSV file.
"""

# Create a DataFrame
df = pd.DataFrame(quotes_data)

# Display the DataFrame
print(df)

# Save the data to a CSV file
df.to_csv('quotes.csv', index=False)
print("Data saved to quotes.csv")

"""Explanation:
* pd.DataFrame(quotes_data) creates a DataFrame from the extracted data.
* df.to_csv('quotes.csv', index=False) saves the DataFrame to a CSV file named quotes.csv.

**Example in Complete way -->**
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL to scrape
url = 'http://quotes.toscrape.com'

# Send a GET request
response = requests.get(url)

# Check if request was successful
if response.status_code == 200:
    soup = BeautifulSoup(response.text, 'html.parser')

    # Extracting quotes, authors, and tags
    quotes_data = []
    quotes = soup.find_all('div', class_='quote')

    for quote in quotes:
        text = quote.find('span', class_='text').text
        author = quote.find('small', class_='author').text
        tags = [tag.text for tag in quote.find_all('a', class_='tag')]

        quotes_data.append({
            'Quote': text,
            'Author': author,
            'Tags': ', '.join(tags)
        })

    # Create DataFrame and save to CSV
    df = pd.DataFrame(quotes_data)
    df.to_csv('quotes.csv', index=False)
    print("Data saved to quotes.csv")
else:
    print(f"Failed to fetch the web page. Status Code: {response.status_code}")

"""**Summary**
* **Fetching Webpage:** We used requests to send an HTTP GET request to the webpage.
* **Parsing HTML:** We used BeautifulSoup to parse and extract specific elements like quotes, authors, and tags.
* **Data Storage:** We stored the extracted data in a pandas DataFrame and saved it to a CSV file for further analysis.



Web scraping can be a powerful tool for collecting data from the web, but always remember to respect the website's --.txt file and terms of service.
"""